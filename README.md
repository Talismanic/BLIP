PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  

<div align="center">
  <img src="assets/blip_logo.png">
</div>

## BLIP: Bootstrapping Language-Image Pre-training

This repository contains code for our paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086). BLIP is a unified vision-language pre-training framework that can perform various vision-language tasks, including image-text retrieval, image captioning, visual question answering, and natural language visual reasoning.

## Features

+ A new vision-language pre-training framework that transfers well to various downstream tasks.
+ A new captioning dataset with 11M images, 5.5M filtered captions, and 1.8M synthetic questions/captions.
+ State-of-the-art results on multiple vision-language tasks.

## Installation

```bash
pip install -r requirements.txt
```

## Download datasets and pretrained models

You can download the datasets and pretrained models from [here](https://github.com/salesforce/BLIP#download-datasets-and-pretrained-models).

## Training

You can find the training instructions [here](https://github.com/salesforce/BLIP#training).

## Evaluation

You can find the evaluation instructions [here](https://github.com/salesforce/BLIP#evaluation).

## License

This project is licensed under the BSD 3-Clause License - see the [LICENSE](LICENSE) file for details.

## Related project: [LAVIS](https://github.com/salesforce/LAVIS)